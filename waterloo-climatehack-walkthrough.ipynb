{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9aba3d",
   "metadata": {},
   "source": [
    "# Hello!\n",
    "This is a walkthrough to train the University of Waterloo's submission model at ClimateHack 2023.\n",
    "\n",
    "To find all our experiments and code, see our original [repo](https://github.com/trevor-yu-087/climatehack.ai-2024), but beware, it is not documented, or well organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b4294",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Environment set up\n",
    "\n",
    "We use docker to package dependencies. If you are using VScode or a JetBrains IDE, the devcontainers extension should be able to use the .devcontainer directory to build the docker image and use it as a development environment.\n",
    "\n",
    "If you do not want to use docker, you can (hopefully) get set up by running:\n",
    "\n",
    "- `pip install -r local-requirements.txt`\n",
    "- `conda install cartopy`\n",
    "- `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`\n",
    "\n",
    "A machine with a CUDA enabled GPU is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958756d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from os import makedirs, path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataset import get_datasets\n",
    "import yaml\n",
    "import torch\n",
    "from pvlib.solarposition import get_solarposition\n",
    "from datetime import datetime\n",
    "from cartopy import crs\n",
    "from itertools import accumulate\n",
    "from functools import partial\n",
    "from perceiver import PVPerceiver\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b545c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Download data\n",
    "Our model used pv, hrv and weather data.\n",
    "\n",
    "For this example we'll only be downloading a few months of data.\n",
    "\n",
    "Note that you have to download the [indices.json](https://github.com/climatehackai/getting-started-2023/blob/main/indices.json) file and place it in the same directory as the data that gets downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/workspaces/waterloo-climatehack/data\" # change this\n",
    "makedirs(datadir, exist_ok=True)\n",
    "\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=\"climatehackai/climatehackai-2023\", \n",
    "    local_dir=datadir, \n",
    "    cache_dir=datadir + '/cache',\n",
    "    local_dir_use_symlinks=False, \n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=[\"aerosols/*\", \"satellite-nonhrv/*\"],\n",
    "    allow_patterns=[\"*10.zarr.zip\", \"*11.zarr.zip\", \"*.parquet\", \"*metadata.csv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614fa250",
   "metadata": {},
   "source": [
    "# Generating PV  Features\n",
    "We generate site specific features (such as the site's max and average output during each month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15636434",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2020, 2021]\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "\n",
    "for i, month_name in enumerate(months):\n",
    "    print(f'Processing {month_name}')\n",
    "    month = pd.read_parquet([datadir + f'/pv/{year}/{i+1}.parquet' for year in [2020,2021]])\n",
    "    month = month.drop(['generation_wh'], axis=1).reorder_levels(['ss_id', 'timestamp'])\n",
    "\n",
    "    site_ids = month.index.get_level_values(0).unique().values\n",
    "\n",
    "    monthly_avg, monthly_max, monthly_average_max = [], [], []\n",
    "\n",
    "    for site in site_ids:\n",
    "        a = month.loc[site].between_time('5:00', '22:00')\n",
    "        monthly_max.append(a.power.max())\n",
    "        monthly_avg.append(a.power.mean())\n",
    "        monthly_average_max.append(a.groupby([a.index.hour, a.index.minute]).power.mean().max())\n",
    "\n",
    "    frame = pd.DataFrame(np.array([monthly_avg, monthly_max, monthly_average_max]).T, index=site_ids)\n",
    "    frame.columns = [f'{month_name}_avg', f'{month_name}_max', f'{month_name}_average_max']\n",
    "\n",
    "    if i == 0:\n",
    "        pv_metrics_frame = frame\n",
    "    else:\n",
    "        pv_metrics_frame = pv_metrics_frame.join(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ddfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metrics_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313ebd3",
   "metadata": {},
   "source": [
    "### Loading PV Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(datadir + '/pv/metadata.csv')\n",
    "metadata.index = metadata.ss_id\n",
    "metadata.drop(['llsoacd', 'operational_at', 'ss_id'], axis=1, inplace=True)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa15ad",
   "metadata": {},
   "source": [
    "### Converting the PV Metrics Dataframe to a Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metric_sites = set(pv_metrics_frame.index)\n",
    "nan_fill = pv_metrics_frame.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_metrics = {}\n",
    "\n",
    "month_names = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "\n",
    "for site_id, (lat, lon, orient, tilt, kwp) in metadata[[\"latitude_rounded\", \"longitude_rounded\", \"orientation\", \"tilt\", \"kwp\"]].iterrows():\n",
    "    for month_number, month in enumerate(month_names, start=1):\n",
    "        key = (lat, lon, orient, tilt, kwp)\n",
    "        metric_names = [\"_\".join([month, metric]) for metric in [\"avg\", \"max\", \"average_max\"]]\n",
    "        if site_id not in pv_metric_sites:\n",
    "            metrics = nan_fill[metric_names].values\n",
    "        else:\n",
    "            metrics = pv_metrics_frame.loc[site_id, metric_names].values\n",
    "        if np.isnan(metrics).any():\n",
    "            metrics = nan_fill[metric_names].values\n",
    "        pv_metrics.setdefault(month_number, {})[key] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir + \"/pv_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pv_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ee367",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Now let's run our dataset class to validate that our data is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml file that is used to configure training runs\n",
    "CONFIG_FILE_NAME = \"train.yaml\"\n",
    "\n",
    "with open(CONFIG_FILE_NAME) as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_datasets(\n",
    "    config[\"data_path\"],\n",
    "    (config[\"start_date\"], config[\"end_date\"]),\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    hrv=\"hrv\" in config[\"modalities\"],\n",
    "    weather=\"weather\" in config[\"modalities\"],\n",
    "    metadata=\"metadata\" in config[\"modalities\"],\n",
    "    seed=config[\"seed\"],\n",
    "    pv_features_file=config[\"pv_features_file\"],\n",
    "    test_size=config[\"test_size\"],\n",
    "    hrv_crop=config[\"hrv_crop\"],\n",
    "    weather_crop=config[\"weather_crop\"],\n",
    "    zipped=config[\"zipped\"],\n",
    "    offset_start_time=config[\"offset_start_time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile solar position\n",
    "_ = get_solarposition(\n",
    "    time=datetime(2020, 1, 2, 3),\n",
    "    latitude=123.4,\n",
    "    longitude=49.0,\n",
    "    method=\"nrel_numba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b10828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrv lat lon features\n",
    "delta_geos = 1000.1343488693237\n",
    "ix = np.arange(config[\"hrv_crop\"]) - (config[\"hrv_crop\"] // 2)\n",
    "xx, yy = np.meshgrid(ix, ix)\n",
    "xx_hrv = xx * delta_geos\n",
    "yy_hrv = yy * delta_geos\n",
    "\n",
    "\n",
    "# weather lat lon features\n",
    "delta_nwp = 0.0623\n",
    "ix = np.arange(config[\"weather_crop\"]) - (config[\"weather_crop\"] // 2)\n",
    "xx, yy = np.meshgrid(ix, ix)\n",
    "xx_nwp = xx * delta_nwp\n",
    "yy_nwp = yy * delta_nwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert between geostationary and lat lon coord systems\n",
    "hrv_coords = crs.Geostationary(central_longitude=9.5, sweep_axis=\"y\")\n",
    "latlon_coords = crs.Geodetic()   \n",
    "def get_hrv_lat_lon_features(lat, lon):\n",
    "    x_geo, y_geo = hrv_coords.transform_point(lon, lat, latlon_coords)\n",
    "\n",
    "    xx_geo = xx_hrv + x_geo\n",
    "    yy_geo = yy_hrv + y_geo\n",
    "\n",
    "    coords = latlon_coords.transform_points(\n",
    "        hrv_coords,\n",
    "        xx_geo, yy_geo\n",
    "    )\n",
    "\n",
    "    xx_lon = coords[..., 0]\n",
    "    yy_lat = coords[..., 1]\n",
    "    features = np.stack((xx_lon, yy_lat), axis=-1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_solar_incidence(az, el, orient, tilt):\n",
    "    # Assume in degrees\n",
    "    panel_vec = np.array([\n",
    "        np.cos(np.radians(orient)),\n",
    "        np.sin(np.radians(orient)),\n",
    "        np.sin(np.radians(tilt))\n",
    "    ])\n",
    "\n",
    "    solar_vec = np.stack([\n",
    "        np.cos(np.radians(az)),\n",
    "        np.sin(np.radians(az)),\n",
    "        np.sin(np.radians(el))\n",
    "    ], axis=1)\n",
    "\n",
    "    sim = -solar_vec @ panel_vec.T\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(id, split_seed: int):\n",
    "    process_seed = torch.initial_seed()\n",
    "    base_seed = process_seed - id\n",
    "    ss = np.random.SeedSequence(\n",
    "        [id, base_seed, split_seed]\n",
    "    )\n",
    "    np_rng_seed = ss.generate_state(4)\n",
    "    np.random.seed(np_rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f18e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_collate_fn(batch):\n",
    "    \"\"\"Data is already batched\n",
    "    Weather is already shape (B, C, L, H, W)\n",
    "    \"\"\"\n",
    "    batch = batch[0]\n",
    "    metadata_features = {}\n",
    "    for (lat, lon, orient, tilt, kwp), t0 in zip(batch[\"metadata\"], batch[\"time\"]):\n",
    "        t0 = pd.Timestamp(t0) - pd.Timedelta(hours=1)\n",
    "        # 60 timestamp including first hour and prediction window\n",
    "        ts = list(accumulate([pd.Timedelta(minutes=5)] * 59, initial=t0))\n",
    "        ts = pd.DatetimeIndex(ts)\n",
    "        solar_pos = get_solarposition(\n",
    "            time=ts, \n",
    "            latitude=lat,\n",
    "            longitude=lon,\n",
    "            method=\"nrel_numba\"\n",
    "        )\n",
    "\n",
    "        # Scale to [0, 1] for SSP\n",
    "        doy = ts.day_of_year.values / 365\n",
    "        mod = ((ts.hour.values * 60) + ts.minute.values) / (24 * 60)\n",
    "        \n",
    "        metadata_features.setdefault(\"time\", []).append(np.stack([\n",
    "            mod,\n",
    "            doy\n",
    "        ], axis=1))\n",
    "\n",
    "        # Weather time features on the hour\n",
    "        t0 = t0.floor(\"60min\")  # t0 already 1 hr before\n",
    "        ts = list(accumulate([pd.Timedelta(minutes=60)] * 5, initial=t0))\n",
    "        ts = pd.DatetimeIndex(ts)\n",
    "        doy = ts.day_of_year.values / 365\n",
    "        mod = ((ts.hour.values * 60) + ts.minute.values) / (24 * 60)\n",
    "        metadata_features.setdefault(\"weather_time\", []).append(np.stack([\n",
    "            mod,\n",
    "            doy\n",
    "        ], axis=1))\n",
    "\n",
    "        lon_xx = lon + xx_nwp\n",
    "        lat_yy = lat + yy_nwp\n",
    "        metadata_features.setdefault(\"location\", []).append(np.stack([\n",
    "            lon_xx,\n",
    "            lat_yy\n",
    "        ], axis=-1))\n",
    "\n",
    "        metadata_features.setdefault(\"hrv_location\", []).append(get_hrv_lat_lon_features(lat, lon))\n",
    "\n",
    "        # Scale to [0, 1] for SSP\n",
    "        az = solar_pos[\"azimuth\"].values / 360\n",
    "        el = solar_pos[\"apparent_elevation\"].values / 360\n",
    "        metadata_features.setdefault(\"azel\", []).append(np.stack([\n",
    "            az,\n",
    "            el\n",
    "        ], axis=1))\n",
    "\n",
    "        # Scale to [0, 1] for SSP\n",
    "        orient = orient / 360\n",
    "        tilt = tilt / 360\n",
    "        metadata_features.setdefault(\"static\", []).append(np.array([\n",
    "            [orient,\n",
    "            tilt,\n",
    "            kwp]\n",
    "        ]))\n",
    "        \n",
    "    batch = {k: torch.FloatTensor(v) for k, v in batch.items() if k not in [\"time\", \"metadata\"]}\n",
    "    for k, v in metadata_features.items():\n",
    "        batch[k] = torch.FloatTensor(np.stack(v))\n",
    "    batch[\"pv\"] = batch[\"pv\"].unsqueeze(-1)\n",
    "    batch[\"pv_features\"] = batch[\"pv_features\"].unsqueeze(-2)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=metadata_collate_fn,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=partial(worker_init_fn, split_seed=0),\n",
    "    num_workers=config[\"num_workers\"]\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    collate_fn=metadata_collate_fn, \n",
    "    pin_memory=True,\n",
    "    worker_init_fn=partial(worker_init_fn, split_seed=0),\n",
    "    num_workers=config[\"num_workers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66535ec9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_loader) * config[\"epochs\"]\n",
    "model = PVPerceiver(\n",
    "    total_steps,\n",
    "    embedding_dim=config[\"dim\"], \n",
    "    num_transformer_layer=config[\"num_layers\"],\n",
    "    nwp_window=config[\"weather_crop\"], \n",
    "    lr=config[\"lr\"], \n",
    "    wd=config[\"wd\"],\n",
    "    init_std=config[\"init_std\"],\n",
    "    num_latents=config[\"num_latents\"],\n",
    "    bias=config[\"bias\"],\n",
    "    pv_latent=config[\"pv_latent\"],\n",
    "    num_pv_features=config[\"num_pv_features\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_dir = path.join(config[\"results_path\"], f\"{timestr}_{config['run_name']}\")\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=save_dir,\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(filename=timestr + \"{epoch}-{step}-{val_mae:.3f}\", monitor=\"val_mae\", mode=\"min\", save_top_k=3),\n",
    "        LearningRateMonitor(logging_interval='step', log_momentum=True)\n",
    "    ],\n",
    "    log_every_n_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a780d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
